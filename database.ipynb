{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Divvy Bike Data: \n",
    "\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202201-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202202-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202203-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202204-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202205-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202206-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202207-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202208-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202209-divvy-publictripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202210-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline 202211-divvy-tripdata.csv<br>\n",
    "mongoimport --type csv -d chicago_bikes -c divvy_ridedata --headerline  202212-divvy-tripdata.csv<br>\n",
    "\n",
    "### Import Weather Data: \n",
    "mongoimport --type csv -d chicago_bikes -c weather_daily --headerline weather_daily.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "import json\n",
    "\n",
    "# Adding for query to find top ten stations \n",
    "#from pymongo.collection import Collection\n",
    "#from pymongo.aggregation import Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MongoClient\n",
    "mongo = MongoClient(port=27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'autosaurus', 'classDB', 'config', 'epa', 'fruits_db', 'gardenDB', 'local', 'met', 'mongodbVSCodePlaygroundDB', 'petsitly_marketing', 'travel_db', 'uk_food']\n"
     ]
    }
   ],
   "source": [
    "# check our list of collections\n",
    "print(mongo.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the database to a variable name\n",
    "db = mongo.chicago_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# review the collections in our new database\n",
    "print(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# review a document in the customer_list collection\n",
    "print(db.divvy_ridedata.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_rides = db['divvy_ridedata']\n",
    "weather_daily = db['weather_daily']\n",
    "withStation = db['withStationName']\n",
    "withoutStation = db['withoutStationName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(divvy_rides.count_documents({}))\n",
    "print(weather_daily.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidOperation",
     "evalue": "No operations to execute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidOperation\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m      9\u001b[0m     bulk_updates\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     10\u001b[0m         pymongo\u001b[39m.\u001b[39mUpdateOne(\n\u001b[0;32m     11\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m: document[\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m]},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m         )\n\u001b[0;32m     19\u001b[0m     )\n\u001b[0;32m     21\u001b[0m \u001b[39m# Execute bulk write operations\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m divvy_rides\u001b[39m.\u001b[39;49mbulk_write(bulk_updates)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pymongo\\_csot.py:108\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[39mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[0;32m    107\u001b[0m             \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 108\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pymongo\\collection.py:571\u001b[0m, in \u001b[0;36mCollection.bulk_write\u001b[1;34m(self, requests, ordered, bypass_document_validation, session, comment, let)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrequest\u001b[39m!r}\u001b[39;00m\u001b[39m is not a valid request\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    570\u001b[0m write_concern \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_concern_for(session)\n\u001b[1;32m--> 571\u001b[0m bulk_api_result \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49mexecute(write_concern, session)\n\u001b[0;32m    572\u001b[0m \u001b[39mif\u001b[39;00m bulk_api_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    573\u001b[0m     \u001b[39mreturn\u001b[39;00m BulkWriteResult(bulk_api_result, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pymongo\\bulk.py:562\u001b[0m, in \u001b[0;36m_Bulk.execute\u001b[1;34m(self, write_concern, session)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute operations.\"\"\"\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mops:\n\u001b[1;32m--> 562\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidOperation(\u001b[39m\"\u001b[39m\u001b[39mNo operations to execute\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    563\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecuted:\n\u001b[0;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidOperation(\u001b[39m\"\u001b[39m\u001b[39mBulk operations can only be executed once.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mInvalidOperation\u001b[0m: No operations to execute"
     ]
    }
   ],
   "source": [
    "documents = divvy_rides.find({})\n",
    "bulk_updates = []\n",
    "\n",
    "for document in documents:\n",
    "    original_value = document[\"started_at\"]\n",
    "    parts = original_value.split(\" \")\n",
    "    date_part = parts[0]\n",
    "    time_part = parts[1]\n",
    "    bulk_updates.append(\n",
    "        pymongo.UpdateOne(\n",
    "            {\"_id\": document[\"_id\"]},\n",
    "            {\n",
    "                \"$set\": {\n",
    "                    \"started_at_date\": date_part,\n",
    "                    \"started_at_time\": time_part\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Execute bulk write operations\n",
    "divvy_rides.bulk_write(bulk_updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidOperation",
     "evalue": "No operations to execute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidOperation\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m      9\u001b[0m     bulk_updates\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     10\u001b[0m         pymongo\u001b[39m.\u001b[39mUpdateOne(\n\u001b[0;32m     11\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m: document[\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m]},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m         )\n\u001b[0;32m     19\u001b[0m     )\n\u001b[0;32m     21\u001b[0m \u001b[39m# Execute bulk write operations\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m divvy_rides\u001b[39m.\u001b[39;49mbulk_write(bulk_updates)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pymongo\\_csot.py:108\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[39mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[0;32m    107\u001b[0m             \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 108\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pymongo\\collection.py:571\u001b[0m, in \u001b[0;36mCollection.bulk_write\u001b[1;34m(self, requests, ordered, bypass_document_validation, session, comment, let)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrequest\u001b[39m!r}\u001b[39;00m\u001b[39m is not a valid request\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    570\u001b[0m write_concern \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_concern_for(session)\n\u001b[1;32m--> 571\u001b[0m bulk_api_result \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49mexecute(write_concern, session)\n\u001b[0;32m    572\u001b[0m \u001b[39mif\u001b[39;00m bulk_api_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    573\u001b[0m     \u001b[39mreturn\u001b[39;00m BulkWriteResult(bulk_api_result, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pymongo\\bulk.py:562\u001b[0m, in \u001b[0;36m_Bulk.execute\u001b[1;34m(self, write_concern, session)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute operations.\"\"\"\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mops:\n\u001b[1;32m--> 562\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidOperation(\u001b[39m\"\u001b[39m\u001b[39mNo operations to execute\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    563\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecuted:\n\u001b[0;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidOperation(\u001b[39m\"\u001b[39m\u001b[39mBulk operations can only be executed once.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mInvalidOperation\u001b[0m: No operations to execute"
     ]
    }
   ],
   "source": [
    "documents = divvy_rides.find({})\n",
    "bulk_updates = []\n",
    "\n",
    "for document in documents:\n",
    "    original_value = document[\"ended_at\"]\n",
    "    parts = original_value.split(\" \")\n",
    "    date_part = parts[0]\n",
    "    time_part = parts[1]\n",
    "    bulk_updates.append(\n",
    "        pymongo.UpdateOne(\n",
    "            {\"_id\": document[\"_id\"]},\n",
    "            {\n",
    "                \"$set\": {\n",
    "                    \"ended_at_date\": date_part,\n",
    "                    \"ended_at_time\": time_part\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Execute bulk write operations\n",
    "divvy_rides.bulk_write(bulk_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(db.divvy_ridedata.find_one())\n",
    "print(db.weather_daily.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_rides.create_index([(\"started_at_date\", 1)])\n",
    "weather_daily.create_index([(\"date\", 1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'weather_daily',\n",
    "            'localField': 'started_at_date',\n",
    "            'foreignField': 'date',\n",
    "            'as': 'weather_data'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$unwind': {\n",
    "            'path': '$weather_data',\n",
    "            'preserveNullAndEmptyArrays': True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$merge': {\n",
    "            'into': 'divvy_ridedata_merged',  # Replace with your new collection name\n",
    "            'whenMatched': 'merge',  # Merge documents with matching _id fields\n",
    "            'whenNotMatched': 'insert'  # Insert documents that don't match\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "divvy_rides.aggregate(pipeline)\n",
    "print(\"Update completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the database and collection\n",
    "collection = db[\"divvy_ridedata_merged\"]  # Replace with your collection name\n",
    "\n",
    "# Count the documents in the collection\n",
    "document_count = collection.count_documents({})\n",
    "print(\"Total number of documents:\", document_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_ridedata_merged = db['divvy_ridedata_merged']\n",
    "divvy_ridedata_merged.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the collections in our new database\n",
    "print(db.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use aggregation pipeline to create a collection that contains start and end station names\n",
    "pipeline = [\n",
    "         {\"$match\": {\"start_station_name\": {\"$exists\": True, \"$ne\": \"\"}, \n",
    "                     \"end_station_name\":{\"$exists\": True, \"$ne\": \"\"}}},\n",
    "\n",
    "         {\"$out\": \"withStationName\"}\n",
    "]\n",
    "\n",
    "# Perform the aggregation\n",
    "result = list(divvy_ridedata_merged.aggregate(pipeline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure that collections is updated\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any documents without start or end station names\n",
    "print(db.withStationName.find_one({\"start_station_name\":\"\"}))\n",
    "print(db.withStationName.find_one({\"end_station_name\":\"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use aggregation pipeline to create a collection that doesn't contain start and end station names\n",
    "pipeline = [\n",
    "         {\"$match\": {\"start_station_name\": {\"$exists\": True, \"$eq\": \"\"}, \n",
    "                     \"end_station_name\":{\"$exists\": True, \"$eq\": \"\"}}},\n",
    "\n",
    "         {\"$out\": \"withoutStationName\"}\n",
    "]\n",
    "\n",
    "# Perform the aggregation\n",
    "result = list(divvy_ridedata_merged.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any documents without start or end station names\n",
    "print(db.withoutStationName.find_one({\"start_station_name\":{\"$ne\":\"\"}}))\n",
    "print(db.withoutStationName.find_one({\"end_station_name\":{\"$ne\":\"\"}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use aggregation pipeline to find top ten start stations \n",
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$start_station_name\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"latitude\": {\"$first\": \"$end_lat\"},\n",
    "            \"longitude\": {\"$first\": \"$end_lng\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 10\n",
    "    },\n",
    "    {   \"$out\": \"Top10StartStations\"\n",
    "}\n",
    "]\n",
    "\n",
    "# Perform the aggregation\n",
    "result = list(withStation.aggregate(pipeline))\n",
    "\n",
    "# Assign collection to a variable\n",
    "Top10StartStations = db['Top10StartStations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see that the collection was added\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top10StartStations.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use aggregation pipeline to find top ten end stations \n",
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$end_station_name\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"latitude\": {\"$first\": \"$end_lat\"},\n",
    "            \"longitude\": {\"$first\": \"$end_lng\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 10\n",
    "    },\n",
    "    {   \"$out\": \"Top10EndStations\"\n",
    "}\n",
    "]\n",
    "\n",
    "# Perform the aggregation\n",
    "result = list(withStation.aggregate(pipeline))\n",
    "\n",
    "# Assign to a variable\n",
    "Top10EndStations = db['Top10EndStations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline query to find the top ten bike routes (by start and end station)\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": { \"Start Station\": \"$start_station_name\", \"End Station\": \"$end_station_name\"},\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"latitude\": {\"$first\": \"$end_lat\"},\n",
    "            \"longitude\": {\"$first\": \"$end_lng\"}\n",
    "        }\n",
    "    },\n",
    "    {\"$sort\": {\"count\": -1}\n",
    "},\n",
    "    {\n",
    "        \"$limit\": 10\n",
    "},\n",
    "    {   \"$out\": \"Top10Routes\"\n",
    "}\n",
    "]\n",
    "# Perform the aggregation\n",
    "result = list(withStation.aggregate(pipeline))\n",
    "\n",
    "# Assign to a variable\n",
    "Top10Routes = db['Top10Routes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a pipeline query to find the top ten bike routes (by start and end lat/lon)\n",
    "# pipeline = [\n",
    "#     {\n",
    "#         \"$group\": {\n",
    "#             \"_id\": { \"Start Station\": \"$start_station_name\", \"End Station\": \"$end_station_name\"},\n",
    "#             \"count\": {\"$sum\": 1},\n",
    "#             \"latitude\": {\"$first\": \"$end_lat\"},\n",
    "#             \"longitude\": {\"$first\": \"$end_lng\"}\n",
    "#         }\n",
    "#     },\n",
    "#     {\"$sort\": {\"count\": -1}\n",
    "# },\n",
    "#     {\n",
    "#         \"$limit\": 10\n",
    "# },\n",
    "#     {   \"$out\": \"Top10Routes\"\n",
    "# }\n",
    "# ]\n",
    "# # Perform the aggregation\n",
    "# result = list(withStation.aggregate(pipeline))\n",
    "\n",
    "# # Assign to a variable\n",
    "# Top10Routes = db['Top10Routes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
